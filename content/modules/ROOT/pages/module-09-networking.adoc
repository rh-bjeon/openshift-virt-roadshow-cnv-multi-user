=  가상 머신의 네트워크 관리

== 소개

이전 섹션에서 언급했듯이, 모든 가상 머신은 기본적으로 OpenShift 소프트웨어 정의 네트워크(SDN)에 연결됩니다. 이를 통해 클러스터 내의 다른 워크로드, 즉 다른 가상 머신이나 OpenShift 네이티브 애플리케이션들과의 통신이 가능하며, 가상 머신과 그 위에서 실행되는 애플리케이션을 보다 현대적인 방식으로 관리할 수 있습니다.

* The SDN provides additional features for abstracting, connecting, and exposing applications in a controlled manner, whether deployed as VMs or Pods in the cluster. These include the *Service* and *Route* features of OpenShift.
* OpenShift's network policy engine allows the VM user or administrator to create rules which allow or deny network traffic to and from individual VMs or entire projects/namespaces.

* SDN은 VM 또는 Pod로 배포된 애플리케이션을 추상화하고 연결하며, 외부에 노출할 수 있는 기능을 제공합니다. 여기에는 OpenShift의 서비스(Service) 및 라우트(Route) 기능이 포함됩니다.
* OpenShift의 네트워크 정책 엔진을 사용하면 VM 사용자나 관리자가 특정 VM이나 프로젝트/네임스페이스 전체에 대한 네트워크 트래픽을 허용하거나 차단하는 규칙을 만들 수 있습니다.

하지만 필요할 경우, 가상 머신이 태그되지 않은 네트워크(untagged network)나 VLAN과 같은 하나 이상의 물리 네트워크에 직접 연결하는 것도 가능합니다. 이는 SDN과 병행해서도 설정할 수 있으며, 예를 들어 관리자는 외부 IP 주소를 통해 VM에 접속할 수 있고, VM들은 Layer 2 네트워크를 사용해 서로 직접 연결할 수도 있습니다.

간단히 말해, 이는 호스트 네트워크(예: Linux bridge)를 설정함으로써 이를 구성합니다. 이 실습에서는 그 다음 단계로, 가상 머신이 해당 브리지에 연결되어 물리 네트워크에 직접 접근할 수 있도록 하는 네트워크 연결 정의(Network Attachment Definition)를 생성하는 방법을 단계별로 안내합니다.

NOTE: 현재 OpenShift 환경에는 가상 머신이 연결될 수 있도록 각 컴퓨트 노드에 Linux 브리지가 이미 구성되어 있으므로, 외부 네트워크 리소스와의 연결을 쉽게 할 수 있습니다.

[[review]]
== 환경 검토

**Kubernetes NMState Operator**는 NMState를 사용하여 OpenShift Container Platform 클러스터 노드 전체에 걸쳐 상태 기반 네트워크 구성을 수행할 수 있는 Kubernetes API를 제공합니다. 사용자는 이 오퍼레이터를 통해 클러스터 노드에서 다양한 네트워크 인터페이스 유형, DNS 및 라우팅을 구성할 수 있으며, 클러스터 노드의 데몬은 각 노드의 네트워크 인터페이스 상태를 주기적으로 API 서버에 보고합니다.

. 왼쪽 메뉴에서 **네트워킹**을 클릭한 다음 **Node network configuration**을 클릭하여 현재 구성을 확인하세요.
+
image::2025_spring/module-09-networking/01_NodeNetworkState_List.png[link=self, window=blank, width=100%]

. 위에서 말씀드린 것처럼, 워커 노드에는 이 모듈에서 사용할 수 있도록 이미 Linux bridge가 구성되어 있습니다. 워커 중 하나를 확장한 후 *br-flat* 브리지를 클릭하여 자세한 정보를 확인합니다.
+
image::2025_spring/module-09-networking/02_NodeNetworkState_Info.png[link=self, window=blank, width=100%]

. 모서리에 있는 X를 클릭하여 브리지 세부 정보를 닫으세요. **br-flat**이라는 이름의 이 브리지는 **Kubernetes NMState Operator**를 사용하여 생성되었습니다. 왼쪽 메뉴에서 **NodeNetworkConfigurationPolicy**를 클릭하여 자세히 살펴보세요.
+
image::2025_spring/module-09-networking/03_NodeNetworkConfigurationPolicy_List.png[link=self, window=blank, width=100%]

. 정보를 얻으려면 **br-flat**을 선택하세요.
+
image::2025_spring/module-09-networking/04_NodeNetworkConfigurationPolicy_Info.png[link=self, window=blank, width=100%]
+
NOTE: **NodeNetworkConfigurationPolicy**는 노드 수준에서 구성을 수행하므로 현재 사용자 계정으로는 이러한 옵션을 수정할 수 없습니다. 따라서 관리자에게 문의하라는 메시지가 표시됩니다.
+
. 이 브리지가 어떻게 생성되었는지 확인하려면 *YAML* 형식으로 전환하여 정의를 볼 수 있습니다. 관리자라면 아래 YAML 코드 조각을 사용하여 유사한 브리지를 생성할 수 있습니다.\
+
image::2025_spring/module-09-networking/05_NodeNetworkConfigurationPolicy_YAML.png[link=self, window=blank, width=100%]

////
[source,yaml]
----
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br-flat
spec:
  desiredState:
    interfaces:
      - bridge:
          options:
            stp:
              enabled: false
          port:
            - name: enp3s0
        description: Linux bridge with enp3s0 as a port
        ipv4:
          dhcp: false
          enabled: false
        name: br-flat
        state: up
        type: linux-bridge
----
////

[[nad]]
== 네트워크 연결 정의 생성

가상 머신에서 Linux Bridge를 사용하려면 **네트워크 연결 정의(Network Attachment Definition, NAD)**를 생성해야 합니다. 이 정의는 OpenShift에 네트워크 정보를 제공하고 가상 머신이 해당 네트워크에 연결할 수 있도록 합니다. 네트워크 연결 정의(NAD)는 생성된 프로젝트 범위 내에서만 적용되며, 해당 프로젝트에 배포된 가상 머신에서만 접근할 수 있습니다. *default* 프로젝트에 네트워크 연결 정의(NAD)를 생성하면 클러스터 전역으로 사용 가능니다. 이를 통해 관리자는 특정 사용자가 어떤 네트워크를 사용할 수 있을지 제어할 수 있습니다.

NOTE: 네트워크 연결 정의(NAD)는 OpenShift에 기존에 구성되어있는 네트워크 장치를 사용하도록 지시합니다. 여기서는 **br-flat**이라는 이름으로 미리 구성된 장치를 사용하므로, 반드시 해당 이름을 그대로 사용해야 합니다. 그렇지 않으면 OpenShift가 VM을 컴퓨팅 노드에 배치할 수 없습니다. OpenShift는 해당 이름의 네트워크 장치가 있는 노드만 사용할 수 있기 때문입니다.

. 왼쪽 메뉴에서 **네트워크**를 선택한 다음 **Network Attachment Definitions**를 선택하고 **NetworkAttachmentDefinition 만들기** 버튼을 클릭합니다.
+
image::2025_spring/module-09-networking/06_NetworkAttachDefinition_Create.png[link=self, window=blank, width=100%]
+

IMPORTANT: 네트워크 연결 정의를 생성할 때, 현재 vmexamples-{user} 프로젝트에 있는지 먼저 확인하세요.

. Complete the form for the *vmexamples-{user}* project as follows, then click *Create network attachment definition*:

*vmexamples-{user}* 프로젝트에 대한 양식을 다음과 같이 작성한 다음 "만들기"를 클릭하세요.

* **Name**: flatnetwork
* **네트워크 유형**: Linux bridge
* **브리지 이름**: br-flat
+
image::2025_summer/module-09-networking/07_NetworkAttachDefinition_Create_Form.png[link=self, window=blank, width=100%]
+

NOTE: 위 양식에는 **VLAN 태그 번호**를 입력하는 칸이 있습니다. 이 번호는 VLAN 태그가 할당된 네트워크에 연결할 때 사용됩니다. 이 실습에서는 태그가 지정되지 않은 네트워크를 사용하므로 VLAN 번호를 입력할 필요가 없습니다.
+
NOTE: 호스트의 단일 Linux 브리지에 여러 개의 VLAN을 연결할 수 있습니다. 그런 경우에는 각 VLAN에 대해 별도의 호스트 인터페이스와 브리지를 생성하는 대신 네트워크 연결 정의(Network Attachment Definition)만 생성하면 됩니다.

. 네트워크 연결 정의의 세부 정보를 살펴보세요. 이 정의는 *vmexamples-{user}* 프로젝트에서 생성되었으므로 다른 프로젝트에서는 사용할 수 없습니다.
+
image::2025_summer/module-09-networking/08_NetworkAttachDefinition_Created.png[link=self, window=blank, width=100%]

[[attach]]
== 가상 머신을 네트워크에 연결
. 왼쪽 메뉴에서 **VirtualMachines**로 이동하여 중앙 열에서 *fedora01* VM을 선택합니다. *설정* 탭을 클릭한 다음 왼쪽의 *네트워크* 탭을 클릭합니다.
+
image::2025_spring/module-09-networking/09_VM_Network_Tab.png[link=self, window=blank, width=100%]

. **[네트워크 인터페이스 추가]**를 클릭하고 표시된 대로 양식을 작성한 다음 **[저장]**을 클릭합니다.
+
image::2025_spring/module-09-networking/10_VM_Network_Attach.png[link=self, window=blank, width=100%]
+

. *동작* 메뉴 또는 *시작* 버튼을 사용하여 가상 머신을 시작하고 *콘솔* 탭으로 전환하여 부팅 과정을 확인하세요.
+
image::2025_spring/module-09-networking/11_VM_Network_Startup.png[]
+
*enp2s0* 인터페이스는 플랫 네트워크(*192.168.64.0/18*) 에서 IP 주소를 할당받습니다. 해당 네트워크에는 DHCP 서버가 있어 네트워크에 IP 주소를 제공합니다.
+
image::2025_spring/module-09-networking/12_VM_Network_Console.png[link=self, window=blank, width=100%]

. 위의 단계를 반복하여 Fedora 02 VM을 동일한 *flatnetwork* 네트워크에 연결하세요.

. 콘솔에서 *ping* 명령어를 사용하여 두 VM(fedora01 및 fedora02) 간의 직접 통신을 시연해 보세요.
+
image::2025_spring/module-09-networking/13_VM_Network_Ping.png[link=self, window=blank, width=100%]

[[udn]]
== 사용자 정의 네트워크

사용자 정의 네트워크(UDN)가 구현되기 전에는 OpenShift Container Platform용 OVN-Kubernetes CNI 플러그인이 기본 네트워크(메인 네트워크)에서 레이어 3 토폴로지만 지원했습니다. Kubernetes 설계 원칙에 따라 모든 Pod는 기본 네트워크에 연결되고, 모든 Pod는 IP 주소를 통해 서로 통신하며, Pod 간 트래픽은 네트워크 정책에 따라 제한됩니다. 이렇게 새로운 네트워크 아키텍처를 학습하는 일은, 기존 가상화 관리자들이 자주 우려 사항으로 꼽는 부분입니다.

UDN의 도입은 사용자 정의 Layer 2, Layer 3, 그리고 localnet 네트워크 세그먼트를 사용할 수 있게 함으로써, Kubernetes 파드 네트워크의 기본 Layer 3 토폴로지에 대한 유연성과 세그먼테이션(분리) 기능을 향상시킵니다. 또한 이러한 세그먼트들은 기본적으로 서로 격리됩니다. 이 세그먼트들은 기본 OVN-Kubernetes CNI 플러그인을 사용하는 컨테이너 파드와 가상 머신에서 기본(Primary) 네트워크 또는 보조(Secondary) 네트워크로 동작합니다. UDN은 매우 다양한 네트워크 아키텍처와 토폴로지를 가능하게 하여, 네트워크의 유연성, 보안, 성능을 강화합니다.

클러스터 관리자는 ClusterUserDefinedNetwork 사용자 정의 리소스(CR)를 활용해, 클러스터 수준에서 여러 네임스페이스에 걸쳐 확장되는 추가 네트워크를 UDN으로 생성하고 정의할 수 있습니다. 또한 클러스터 관리자 또는 클러스터 사용자는 UserDefinedNetwork CR을 사용해 네임스페이스 수준에서 UDN으로 추가 네트워크를 정의할 수 있습니다.

사용자 정의 네트워크는 다음과 같은 이점을 제공합니다.

**보안을 위한 향상된 네트워크 격리**: 네임스페이스는 Red Hat OpenStack Platform(RHOSP)에서 테넌트가 격리되는 방식과 유사하게, 자체적으로 격리된 기본(Primary) 네트워크를 가질 수 있습니다. 이는 테넌트 간 트래픽이 섞일 위험을 줄여 보안을 강화합니다.

**네트워크 유연성**: 클러스터 관리자는 기본 네트워크를 Layer 2 또는 Layer 3 유형으로 구성할 수 있습니다. 이를 통해 기본 네트워크에도 보조(Secondary) 네트워크 수준의 유연성을 부여할 수 있습니다.

**단순화된 네트워크 관리**: 사용자 정의 네트워크를 사용하면, 워크로드를 서로 다른 네트워크로 그룹화하여 격리할 수 있으므로 복잡한 네트워크 정책이 필요 없어집니다.

**고급 기능**: 사용자 정의 네트워킹 기능을 통해 관리자는 여러 네임스페이스를 하나의 네트워크에 연결하거나, 네임스페이스 집합별로 서로 다른 네트워크를 만들 수 있습니다. 또한 사용자는 서로 다른 네임스페이스와 클러스터에 걸쳐 IP 서브넷을 지정하고 재사용할 수 있어, 일관된 네트워킹 환경을 제공할 수 있습니다.

== OpenShift Virtualization에서의 사용자 정의 네트워크(UDN)

OpenShift Container Platform 웹 콘솔 또는 CLI를 사용하여, 가상 머신(VM)의 기본(Primary) 인터페이스를 사용자 정의 네트워크(UDN)에 연결할 수 있습니다. 기본 사용자 정의 네트워크는 지정한 네임스페이스에서 기본 Pod 네트워크를 대체합니다. Pod 네트워크와 달리, 기본 UDN은 프로젝트(프로젝트=네임스페이스)별로 정의할 수 있으며, 각 프로젝트는 자체 서브넷과 토폴로지를 사용할 수 있습니다.

Layer 2 토폴로지에서는 OVN-Kubernetes가 노드 간 오버레이(overlay) 네트워크를 생성합니다. 이 오버레이 네트워크를 사용하면 추가적인 물리 네트워킹 인프라를 구성하지 않아도 서로 다른 노드에 있는 VM들을 연결할 수 있습니다.

Layer 2 토폴로지는 라이브 마이그레이션 중에도 클러스터 노드 전반에서 영구 IP 주소가 유지되므로, 네트워크 주소 변환(NAT) 없이 VM을 원활하게 마이그레이션할 수 있게 합니다.

기본 UDN을 구현하기 전에 다음 제한 사항을 고려해야 합니다.

. `virtctl ssh` 명령으로 VM에 대한 SSH 접근을 구성할 수 없습니다.

. `oc port-forward` 명령으로 VM으로 포트를 포워딩할 수 없습니다.

. 헤드리스 서비스(headless service)를 사용해 VM에 접근할 수 없습니다.

. VM 상태 확인(Health Check)를 구성하기 위한 readiness/liveness 프로브를 정의할 수 없습니다.

NOTE: 현재 OpenShift Virtualization은 보조(Secondary) 사용자 정의 네트워크를 지원하지 않습니다.

=== 사용자 정의 네트워크 사용하기

UDN에 접근할 수 있는 파드를 생성하기 전에, 반드시 네임스페이스와 네트워크를 먼저 생성해야 합니다. 파드가 있는 네임스페이스를 새 네트워크에 할당하거나, 기존 네임스페이스에 UDN을 생성하는 작업은 OVN-Kubernetes에서 허용되지 않습니다.

이 작업은 클러스터 관리자가 수행해야 합니다. **vmexamples-{user}-udn**이라는 네임스페이스가 올바른 레이블(`k8s.ovn.org/primary-user-defined-network`)과 함께 이미 할당되어 있습니다.

. **네트워킹**으로 이동한 다음 **UserDefinedNetworks**를 클릭하고, **vmexamples-{user}-udn** 프로젝트를 선택했는지 확인합니다.
+
image::2025_spring/module-09-networking/14_UDN_List.png[link=self, window=blank, width=100%]

. **만들기**를 클릭하고 **UserDefinedNetwork**를 선택합니다.
+
image::2025_spring/module-09-networking/15_UDN_Create.png[link=self, window=blank, width=100%]

. 서브넷으로 **192.168.254.0/24**를 지정한 뒤 **만들기**를 누릅니다.
+
image::2025_spring/module-09-networking/16_UDN_Form.png[link=self, window=blank, width=100%]

. 방금 생성한 UDN의 구성을 검토합니다.
+
image::2025_spring/module-09-networking/17_UDN_Created.png[link=self, window=blank, width=100%]
+

* 폼에서 생성할 때 기본 이름은 **primary-udn**입니다.
* 기본값은 Layer 2이며(현재 OpenShift Virtualization에서 지원되는 유일한 레이어입니다).
* Role은 primary입니다(현재 가상 머신은 primary 네트워크만 사용할 수 있습니다).
* Network Attachment Definition(NAD)이 자동으로 생성됩니다.

. 이제 왼쪽 메뉴에서 **NetworkAttachmentDefinitions**로 이동하여, 연결된 **NAD**가 자동으로 생성된 것을 확인합니다.
+
image::2025_spring/module-09-networking/18_UDN_NAD.png[link=self, window=blank, width=100%]

. UserDefinedNetwork에 연결된 가상 머신을 생성하려면 YAML 정의에서 일부 https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/virtualization/networking#virt-connecting-vm-to-primary-udn[조정(adjustment)^] 이
 필요합니다. 이 실습에서는 작업을 더 쉽게 하기 위해, 아래 YAML 정의를 사용해 UserDefinedNetwork에 연결된 VM 전체를 그대로 정의하겠습니다.

. 다음 이미지에 보이는 것처럼 상단 메뉴를 사용해 YAML을 가져올(import) 수 있습니다.
+
image::2025_spring/module-09-networking/19_UDN_Import_YAML.png[link=self, window=blank, width=100%]
+
[source,yaml,role=execute,subs="attributes"]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: fedora-udn
  name: fedora-udn
  namespace: vmexamples-{user}-udn
spec:
  dataVolumeTemplates:
    - apiVersion: cdi.kubevirt.io/v1beta1
      kind: DataVolume
      metadata:
        creationTimestamp: null
        name: fedora-udn
      spec:
        sourceRef:
          kind: DataSource
          name: fedora
          namespace: openshift-virtualization-os-images
        storage:
          resources:
            requests:
              storage: 30Gi
  runStrategy: Always
  template:
    metadata:
      name: fedora-udn
      namespace: vmexamples-{user}-udn
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          interfaces:
          - name: primary-udn
            binding:
              name: l2bridge
          rng: {}
        resources:
          requests:
            memory: 2048M
      networks:
      - pod: {}
        name: primary-udn
      terminationGracePeriodSeconds: 0
      volumes:
      - dataVolume:
          name: fedora-udn
        name: rootdisk
      - cloudInitNoCloud:
          userData: |-
            #cloud-config
            user: fedora
            password: fedora
            chpasswd: { expire: False }
        name: cloudinitdisk
----

. YAML을 붙여넣은 뒤, 아래쪽의 파란색 **만들기** 버튼을 클릭하면 VM 생성 프로세스가 시작됩니다.
+
image::2025_spring/module-09-networking/20_Create_VM_YAML.png[link=self, window=blank, width=100%]

. **VirtualMachines**로 전환한 뒤 VM이 생성되는 과정을 확인합니다. VM이 실행되면 새로 생성된 **fedora-udn** 가상 머신을 검토합니다. **개요** 탭에서 **네트워크** 타일에 UserDefinedNetwork에서 할당된 IP가 표시됩니다.
+
image::2025_spring/module-09-networking/21_UDN_Network_Tile.png[link=self, window=blank, width=100%]

. 콘솔 탭으로 전환하고 제공된 게스트 자격 증명을 사용해 VM에 로그인합니다.
+
image::2025_spring/module-09-networking/22_UDN_Fedora_Console.png[link=self, window=blank, width=100%]
+
.. VM에는 정의된 서브넷에서 IP가 할당되었습니다.
.. VM은 DHCP로부터 게이트웨이 구성을 자동으로 받습니다.
.. VM은 사용자 정의 네트워크(User Defined Network)를 통해 인터넷에 접근할 수 있습니다.

== 요약

이 모듈에서는 물리 네트워크를 사용하고, 가상 머신(VM)을 기존 네트워크에 직접 연결하는 방법을 살펴보았습니다. VM을 물리 네트워크에 직접 연결하면 관리자가 VM에 직접 접근할 수 있을 뿐만 아니라, 스토리지 네트워크나 관리(Administration) 네트워크 같은 특수 목적 네트워크에 VM이 연결되도록 할 수도 있습니다.

사용자 정의 네트워크는 클러스터 관리자와 최종 사용자에게 매우 높은 수준의 맞춤형 네트워크 구성 옵션을 제공하며, 기본(Primary) 및 보조(Secondary) 네트워크 유형을 모두 관리하는 데 훨씬 더 유연한 경험을 제공합니다.